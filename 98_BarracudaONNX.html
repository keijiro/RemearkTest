<!DOCTYPE html>
<html>
  <head>
    <title>98 Barracuda ONNX</title>
    <meta charset="utf-8">
    <style>
      .remark-slide-content {
        color: white;
        background-color: black;
        font-size: 50pt;
        padding: 0px 0.5em;
      }
      .remark-code-line { font-size: 25pt; margin: 0.2em 0px; }
      p { margin: 0.5em 0px; line-height: 120%; }
      code { margin: 0px 2em; }
    </style>
  </head>
  <body>
<textarea id="source">

皆さんこんにちは。

Unity Technologies Japan の Developer Adovocate の高橋です。

---

この講演では、Unity が提供する machine learning パッケージの一つである Barracuda を紹介します。

また、そこで利用される ONNX というエコシステムについても簡単に解説します。

---

まず最初に、Barracuda が何なのか、ざっくりと解説しておきましょう。

Barracuda とは、すごく簡単に言うと、training 済みの machine learning モデルを実行するための、runtime engine です。

---

Barracuda は、またの名前を "Unity Inference Engine" と言います。

その名前が表している通り、machine learning モデルの inference を専門に扱います。

---

Machine learning は "training" と "inference" という２つのステップに大きく分けられますが、そのうちの inference だけを行うということですね。

---

Training については、他のパッケージを使って済ませておくことが前提となります。

---

さて、言葉だけで説明していてもなかなかイメージが掴みにくいと思うので、実際に動いている所を見てもらいましょう。

---

まずはオブジェクト検出モデルとして有名な YOLO を動かしてみました。

具体的には YOLOv4 の Tiny モデルになります。

この PC は GeForce RTX 2060 SUPER を搭載していますが、だいたい 240 fps ぐらいで動いています。

---

これは MediaPipe の FaceMesh パイプラインです。

顔検出、ランドマーク検出、瞳検出などの、複数のモデルを組み合わせることで、フェーシャルトラッキングを実現しています。

---

次は同じく MediaPipe から Selfie モデルです。

これは、いわゆる human segmentation を行うモデルで、人体の切り抜きマスクを高速に生成します。

---

これを使えば、ビデオチャットのバーチャル背景のようなものを簡単に実現できますね。

このモデルは非常に軽量で、PC 上では 400 fps 以上で動いています。

---

同じプロジェクトを、少し古めの iPhone X 上で動かしてみました。

こちらも 60fps で動かすことができています。

---

このように様々なプラットフォームに柔軟に対応できるのは Unity と Barracuda の強みと言えますね。

---

では、さっそく Barracuda の解説に……と行きたいところなんですが、その前に ONNX についてお話ししておきましょう。

---

O-N-N-X, Open Neural Network Exchange, ONNX は、ものすごく簡単に言うと、machine learning モデルの共通ファイルフォーマットです。

---

Machine learning には TensorFlow や PyTorch など様々なライブラリがありますが、ファイル形式に互換性はありません。

---

そこで登場するのが ONNX です。

生成されたモデルを ONNX 形式で出力することによって、様々なランタイムで使いまわすことができるようになります。

---

そして Barracuda はこの ONNX ファイル形式に対応しています。

---

つまり、TensorFlow や PyTorch で生成したモデルを ONNX 形式で出力すれば、それを Unity へインポートすることができます。

そして、Unity が対応する様々なプラットフォームに展開することが可能になる……と言う理屈ですね。

---

実際には、これを実現するのは口で言うほど簡単ではないんですが、ひとまず、そういう理想を持って作られている、と理解してもらえればいいかなと思います。

---

では次に、簡単なサンプルを使って、実際に ONNX ファイルを Barracuda にインポートする工程をお見せしましょう。

ここにサンプルとして用意した小規模な ONNX ファイルがあります。

---

中身はこんな感じです。

足し算、掛け算、convolution など、基本的なオペレーションを一通り通しているだけの、簡単なネットワークですね。

---

これを Unity にドラッグ＆ドロップします。

これでインポート完了です。

---

Barracuda は Netron に対応していて、インポートしたファイルをダブルクリックで開けます。

また、インスペクタからこのボタンを押すことで、 Barracudaの 内部表現を確認することもできます。

---

ちなみに、これは Netron と言うツールです。

Neural network の視覚化を行なってくれる、非常に便利なツールです。

---

ONNX 以外の形式にも対応していて、デバッグ作業などで非常に役立ちます。

Barracuda を使用する際には是非インストールしておいてください。

---

さて、こうして ONNX ファイルのネットワークと、 Barracuda の内部表現を比較してみると、構造がだいぶ異なっていることが分かると思います。

Barracuda はインポート時に最適化を行うので、こんな風に異なった内部表現に変化することがあるんですね。

---

具体的には、事前計算可能な部分が結合されていたりします。

２回行ってる transpose が不要になるように調整されてもいますね。

あと、convolution と bias と activator が一括化されていたりもします。

---

次は、実際に Barracuda を使用するサンプルコードを見てみましょう。

---

これは machine learning のチュートリアルとしてよく使われる、MNIST 手書き認識モデルのサンプルです。

こちらのリポジトリで公開しています。

---

主なコードは MnistTest.cs 内にあります。

中身はこんな感じです。

---

Barracuda の API は、ごく普通の作りをしているので、この手のライブラリを使ったことのある方なら、難なく使いこなせるかもしれません。

---

簡単に流れを見ていきましょう。

まず、入力用の tensor を作成します。

Tensor というのは、簡単に言ってしまえば、多次元の数値配列のことですね。

---

Unity でグラフィックスを扱う際には、4x4 要素の２次元マトリクスなどがよく使われますが、machine learning の分野では、512x512x3 の３次元 tensor とか、更に大きな４次元 tensor などがよく使われます。

---

そういった巨大な数値配列を効率的に扱うための仕組みが用意されていると理解してください。

ただ、ここで使ってるのは 28x28 の比較的小さな tensor ですね。

MNIST は小さなモデルなので、この程度の要素数で動かします。

---

ちなみに Tensor クラスは IDisposable の継承クラスです。

使わなくなったタイミングで明示的に Dispose メソッドを呼ぶか、あるいはこのように using ステートメントを使う必要があります。

---

Tensor の中身は普通の C# の多次元配列のようにアクセスできます。

入力画像のグレースケール値を１ピクセルずつ代入していますね。

---

ただ、画像の表現形式にちょっとした違いがあって、ここで Y 軸を反転させておく必要がありました。

このような表現形式の変換は、必ずと言っていいほど必要とされるので、よく覚えておいてください。

---

次に machine learning モデルを動かすための worker instance を作成します。

デバイスの種類としては CPU を指定しています。

---

ちなみに、ここを GPU に変更すれば、Compute Shader を利用して動く GPU worker を生成することもできます。

このモデルは非常に小規模なので、CPU worker でも十分でしょう。

---

Execute メソッドで実行したら、PeekOutput メソッドで結果を取り出します。

最後に、取得された値に対して SoftMax 関数を適用します。

---

SoftMax 関数は、machine learning でよく使われるお決まりの関数ですね。

このモデルの説明に、結果は SoftMax を使ってください、と書かれていたので、その通りにしました。

---

結果はこんな感じです。

ちゃんと手書きの数値が認識されているのが分かると思います。

---

以上が、最も簡単な Barracuda の使用例でした。

C# スクリプトを使って非常に簡単に machine learning モデルを動かすことができる、というのが分かると思います。

---

ただ、このような画像のプリプロセスを、シングルスレッドのナイーブな C# スクリプトで動かすのは、実際にはちょっと心許ないですよね。

---

この MNIST のモデルは、入力がたかだか 28x28 なので、まだ許容できます。

ただ、例えばこれが 512x512x3 のような巨大な tensor になってくると、話が変わってきます。

---

そのような場合は Compute Shader を使って GPU 側でプリプロセスを行った方がいいでしょう。

---

そこで、同じ MNIST のモデルを、なるべく GPU 側でパイプラインを動かすよう、実装し直してみることにしました。

このサンプルはこちらのリポジトリで公開しています。

---

さっそくソースコードを覗いてみましょう。

まず、入力 tensor を格納するためのデータ領域として ComputeTensorData を作成しています。

---

ComputeTensorData は Compute Buffer を使ってデータを格納するためのクラスです。

前のサンプルでは、デフォルトのデータ領域として CPU 側のメモリを使用していました。

---

今回はこのように明示的にデータ領域を指定することで、GPU 側のメモリを使用します。

ChannelsOrder は Compute Buffer 上での要素の並び順を指定するオプションです。

画像処理の場合は大抵 NHWC を使うことになると思ってください。

---

次に、入力画像を tensor へコピーするための compute shader を実行します。

Compute shader の中身はこんな感じです。

---

簡単な内容なので、詳しい解説は省略します。

ただ、やはりここで画像の上下を反転していますね。

---

あと、出力先となる compute buffer は１次元配列としてしかアクセスできないので、インデクスの計算が若干ややこしくなっています。

---

入力 tensor が用意できたら、worker instance を作成して、実行します。

今回は GPU で実行することが前提なので、worker の種類として GPU を指定しました。

---

今回は inference の結果も compute buffer として参照します。

出力 tensor のデータ領域を tensorOnDevice プロパティで取得して、ComputeTensorData にダウンキャストする、という経路を使います。

---

Inference の結果は 10 要素の tensor となるので、それを格納するための compute buffer を作成します。

そして、ポストプロセスを行うための compute shader を実行します。

---

Compute shader の中身はこんな感じです。

これはもう本当に SoftMax 関数を適用しているだけですね。

---

結果を格納した compute buffer は、マテリアルを介してそのまま描画用のシェーダーへ渡されます。

こうすることで、CPU を経由すること無く、そのまま描画パイプラインへと流れていきます。

---

つまり、入力から出力まで、一切 CPU 側にデータを戻すことなく、GPU 上だけで処理を完結させることができるというわけです。

---

こうすることで、CPU 側の負荷に引きずられることなく、GPU 側で高速に machine learning モデルを実行することができます。

---

まあ実際には、inference の結果を CPU 側に戻さずに完結させるというユースケースは、あんまり無いかもしれません。

ただ、やろうと思えばここまでできる、という感じですね。

---

さて、ここまで、MNIST のサンプルを CPU 側で動かす方法と、GPU 側で動かす方法と、２種類のアプローチを見てきました。

ここで Barracuda のバックエンドの構成について簡単に説明しておきたいと思います。

---

Barracuda で inference 処理を行うバックエンドの実装には、次の６種類があります。

CSharpRef は、ナイーブな C# で書かれたリファレンス実装です。

これは主にテスト用の実装なので、みなさんが使う機会は無いと思います。

---

CSharp は高速な C# 実装です。

WebGL を含む全てのプラットフォームで動作するので、なるべく多くのプラットフォームをカバーしたい場合に使います。

ビルド時は IL2CPP による高速化が期待できます。

---

CSharpBurst は Burst コンパイラを使ったバックエンドです。

CPU 側で動かす場合は、これがもっとも高速な実装になりますが、 Burst 非対応のプラットフォームでは使えません。

例えば WebGL などは非対応です。

---

ComputeRef は GPU 用のリファレンス実装です。これも皆さんが使う機会は無いと思います。

GPU 上で動く実装として Compute と ComputePrecompiled がありますが、この２つは GPU 上でのパフォーマンスには差がありません。

---

ComputePrecompiled は CPU 側の負荷を減らすための最適化を含んでいます。

通常はこちらを使用してください。

---

なお、これらの GPU 実装を動かすには、プラットフォームが Compute Shader に対応している必要があります。

---

PC や Mac であれば、まず確実に対応しているはずです。

モバイルの場合は、Metal や Vulkan であれば動作しますが、OpenGL ES では動かない可能性もあります。

---

まとめると、比較的パワーのある環境では ComputePrecompiled を使用し、OpenGL ES 等を含める場合は CSharpBurst を使用し、WebGL まで含める場合は CSharp を使用する、と言うような順序で選択することになるかと思います。

---

ここから、僕が Barracuda 上で動作を確認したいくつかのモデルを紹介したいと思います。

---

まず最初は、アニメ調のイラストに特化した super resolution フィルタとして有名な Waifu2x です。

古典的ですが、いまだに強い人気のあるフィルタですね。

---

次は BodyPix という、人体に特化した semantic segmentation を行うモデルです。

ポーズ解析のためのキーポイント検出も備えていて、何にでも使えるとても便利な万能モデルです。

---

このモデルについては以前、別の動画で解説を行いました。

興味のある方は是非チェックしてみてください。

---

冒頭でも紹介した MediaPipe の Selfie モデルです。

バーチャル背景の実装などに使えます。

とても軽量なのが特徴ですね。

---

これは M-LSD という、line segment detection モデルです。

任意の画像から、線分の要素を抽出することができます。

ワイヤフレーム的な構造に変換できるというわけですね。

---

これは冒頭でも紹介した YOLOv4 tiny モデルです。これともう一つ……

---

YOLOv2 の tiny モデルも Barracuda で動かしてみました。

YOLOv2 の方が精度は低いですが、 YOLO シリーズの中ではもっとも軽量なモデルなので、用途次第ではまだ役に立つかもしれません。

---

これは Ultra Face と言う顔検出フィルタです。

大量の顔をかなり正確に検出できます。

---

こちらの BlazeFace はもう少し一般的な顔検出です。

非常に高速に動作するので、この後に紹介する顔解析モデルのプリプロセスとして使えます。

---

これは MediaPipe の顔のランドマークを検出するモデルです。

顔の動きの分析に使用できます。

---

こちらは瞼と瞳の分析に特化したモデルですね。

そしてこれらのモデルを組み合わせることによって……

---

フェーシャルトラッキングのパイプラインが実現できます。

このサンプルは GPU 上で処理が完結するように実装してあります。

そのおかげで、非常に高速な動作が可能になっています。

---

この BlazePalm は手のひらを検出するモデルですね。

BlazeFace と同じく、プリプロセスとして使える軽量なモデルです。

---

そしてこれは手のランドマークを検出するモデルです。

これらのモデルを組み合わせることによって……

---

ハンドトラッキングのパイプラインが実現できます。

これも GPU 上で動作するように実装してあります。

---

以上、色んなモデルを紹介しましたが、僕の個人的な好みから computer vision 系のモデルに偏ってしまっています。

---

ですが、自然言語処理や音声分析処理などにも、同じように使用できるはずです。

今後機会があれば、それらのモデルも試してみたいと考えています。

---

このように夢のある Barracuda と ONNX の話をしてきたわけですが、現実的な話をすると、そうおいしいことばかりではありません。

まず大きな障害として、ONNX の互換性の問題があげられます。

---

ONNX は非常に表現力の高いフォーマットである反面、完全な互換性の実現が難しいというデメリットがあります。

---

Barracuda の互換性にも限界があって、すべての ONNX モデルが想定通りに動いてくれるわけではありません。

---

というか、既存の pre-trained モデルを使おうとすると、うまく行かないケースの方が、むしろ多いです。

一発で動いてくれるのは、むしろ稀なケースであると言えます。

---

互換性が破綻する理由は色々あります。

まず、ONNX の opset バージョンが異なるためにインポートに失敗するというケースがあります。

---

また、Barracuda は ONNX で定義されている全てのオペレーターに対応しているわけではありません。

未対応オペレーターを使われると、やはりインポートに失敗します。

同様に、カスタムオペレーターにも対応できません。

---

もちろんこの他にも、細かな未実装機能や、単なるバグなども残されています。

ONNX と Barracuda を使う上では、こういった様々な問題に出くわすことになるでしょう。

---

具体的な例を見てみましょう。

これは先ほど紹介した M-LSD 線分検出モデルですが、互換性の問題があって、当初 Barracuda で使うことができませんでした。

---

まず最初の問題は Split オペレーターの仕様です。

ONNX の Split オペレーターは、tensor を任意の配分で分割する機能です。

分割の配分は split アトリビュートを使って指定します。

---

最新の ONNX の仕様では、この split アトリビュートを省略した場合は、デフォルト値で補ってくれるようになっています。

---

ただ Barracuda はこのデフォルト挙動に対応していません。

split アトリビュートを明示的に指定する必要があります。

そのためエラーになってしまっていました。

---

もう一つの問題はネットワークの後半にある TopK オペレーターです。

現状の Barracuda は一応 TopK に対応しています。

---

ただ残念ながら、実装が完全ではなく、エラーを起こすことがあります。

今回もこれを正常に動作させることができませんでした。

---

と言うことで、この M-LSD は Barracuda では動きません……となってしまうのですが、頑張って対処すれば動かすことも可能です。

例えば Split オペレーターの問題については、アトリビュートを追加すれば解決できます。

---

Python の ONNX モジュールを使って、これを修正するコードを組んでみました。

単純にグラフをスキャンして、Split オペレーターを探して、アトリビュートを追加するだけの内容です。

---

次は TopK オペレーターの問題です。

Netron でネットワークを覗いてみると、 TopK はここにあります。

---

これは恐らく、スコアを基準として検出候補の絞り込みを行う処理です。

これより後のサブネットワークは、出力データの整形を行っているだけですね。

---

この手のポストプロセスは Compute Shader で簡単に書けます。

というか、そもそも TopK を使わなくても、閾値によるフィルタリングと NMS で十分な気もします。

そこで、その方向で改造することにしました。

---

ノードの削除にも Python の ONNX モジュールを使いました。

TopK 以降のサブネットワークを削除する処理です。

ちょっとややこしくて、ぱっと見では分かりにくいかもしれません。

---

ONNX ファイルは基本フォーマットとして Protocol Buffers を採用しています。

Protocol Buffers を使いこなしている人なら、もっと簡単に書く方法が思いつくかもしれません。

僕が適当に書くとこうなりました。

---

こんな感じで書いた Python を Google Colab 上で動かしてコンバートしました。

これでようやくBarracuda 上で使えるようになったというわけです。

---

こんな感じで、調整すれば動くんだけど、未調整で動かそうとすると、問題を生じることが多い、というのが Barracuda の現状です。

---

独自のモデルを構築する際には、使用する機能が Barracuda で対応済みかどうかを、前もって確認しておく必要があるでしょう。

また、pre-trained モデルを使用する際には、今回やったような調整が必要になるかもしれません。

---

このように色々課題もあるんですが、ONNX のような共通規格で出力された pre-trained モデルを、マルチプラットフォームに展開できて、しかも高速に動かすことができるというのは、大きな魅力です。

---

アプリ開発において machine learning モデルの利用を考える際には、簡易的に使用できるソリューションのひとつとして、候補に入れてもらえればと思います。

</textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create({
        ratio: '16:9',
        highlightStyle: 'sunburst',
        slideNumberFormat: ''
      });
    </script>
  </body>
</html>
